# Kaggle Competition: Predict the Introverts from the Extroverts

Этот репозиторий содержит решение для соревнования Kaggle **"Predict the Introverts from the Extroverts"**. Целью задачи было предсказать личностный тип (интроверт/экстраверт) на основе предоставленных данных.

Решение показало высокую эффективность и вошло в **топ 5%** (Private Leaderboard) участников.

### Результаты

| Score Type | Score     |
| :--------- | :-------- |
| **Public Score**  | `0.961943` |
| **Private Score** | `0.973279` |

---
## Описание решения

### 1. Предобработка данных и Feature Engineering

Первым этапом была тщательная очистка и подготовка данных:
*   **Обработка пустых значений (NaN):** Для всех признаков были применены стратегии импутации, соответствующие природе данных (медиана, мода, специальное значение или прогнозирующая модель).
*   **Кодирование категориальных признаков:** Использовалось целевое кодирование (Target Encoding) для избежания проблемы "проклятия размерности" по сравнению с One-Hot Encoding.

### 2. Построение моделей

#### **Модель 1: Блендинг разнородных моделей (Лучшее решение)**
Финализированное решение представляет собой блендинг (усреднение предсказаний) трех классических алгоритмов машинного обучения:
*   **Логистическая регрессия (Logistic Regression):** Линейная модель, хороша как бейзлайн.
*   **K-Nearest Neighbors (KNN):** Модель, основанная на расстоянии между точками данных.
*   **Gradient Boosting (XGBoost):** Мощная нелинейная модель, часто показывающая высокие результаты на табличных данных.

**Результат:** Ансамбль этих моделей показал наилучшую обобщающую способность и стабильность на публичном и приватном лидерборде.

#### **Модель 2: Ансамбль из CatBoost моделей**
Альтернативное решение, включающее более сложный ансамблинг:
*   **Байесовская оптимизация:** Для каждой из 5 моделей CatBoost гиперпараметры были подобраны с помощью продвинутого метода оптимизации (Bayesian Optimization).
*   **Обучение на 5 фолдах:** Каждая модель была обучена на 5 различных фолдах (Stratified K-Fold) для уменьшения дисперсии предсказаний.
*   **Блендинг:** Итоговое предсказание — это усреднение вероятностей, полученных от всех 5 моделей.

Несмотря на более сложную архитектуру, данная модель показала чуть худшую результативность по сравнению с первой.
*   `pandas`, `n
